{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56f5b69c-69de-4ad1-95ea-441cdbce1ccd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install databricks=0.2 databricks-langchain==0.5.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46c5435a-8a7b-4c5f-beaf-eafdc5954ba4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import xml.etree.ElementTree as ET\n",
    "from typing import Any, Dict, List\n",
    "from schemas.nifi_metadata import processor_schema, processor_properties_schema, processor_connections_schema\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "catalog = \"sdickey\"\n",
    "schema = \"nifi_workflow_metadata\"\n",
    "\n",
    "# from tools.xml_tools import parse_nifi_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f50fc08-c213-434a-8090-a3087b5e66cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def _trim(s: str | None) -> str:\n",
    "    \"\"\"Trim helper that returns '' for None.\"\"\"\n",
    "    return (s or \"\").strip()\n",
    "\n",
    "\n",
    "def parse_nifi_template_impl(xml_content: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Parse a NiFi XML template and extract processors, properties, and connections.\n",
    "    Returns Python dict for programmatic use.\n",
    "\n",
    "    Parameters:\n",
    "        xml_content: The raw NiFi XML content\n",
    "\n",
    "    Returns:\n",
    "        Dict with processors, connections, counts, and process groups\n",
    "    \"\"\"\n",
    "    root = ET.fromstring(xml_content)\n",
    "\n",
    "    processors: List[Dict[str, Any]] = []\n",
    "    connections: List[Dict[str, Any]] = []\n",
    "\n",
    "    # Build process group mapping for enhanced task naming\n",
    "    process_groups = {}\n",
    "    for group in root.findall(\".//processGroups\"):\n",
    "        group_id = group.findtext(\"id\")\n",
    "        group_name = group.findtext(\"name\") or \"UnnamedGroup\"\n",
    "        if group_id:\n",
    "            process_groups[group_id] = group_name\n",
    "\n",
    "    # Extract processors with process group context\n",
    "    for processor in root.findall(\".//processors\"):\n",
    "        parent_group_id = processor.findtext(\"parentGroupId\")\n",
    "        parent_group_name = (\n",
    "            process_groups.get(parent_group_id, \"Root\") if parent_group_id else \"Root\"\n",
    "        )\n",
    "\n",
    "        proc_info = {\n",
    "            \"name\": _trim(processor.findtext(\"name\") or \"Unknown\"),\n",
    "            \"type\": _trim(processor.findtext(\"type\") or \"Unknown\"),\n",
    "            \"id\": _trim(processor.findtext(\"id\") or \"Unknown\"),\n",
    "            \"properties\": {},\n",
    "            \"parentGroupId\": parent_group_id,\n",
    "            \"parentGroupName\": parent_group_name,\n",
    "        }\n",
    "\n",
    "        props_node = processor.find(\".//properties\")\n",
    "        if props_node is not None:\n",
    "            for entry in props_node.findall(\"entry\"):\n",
    "                k = entry.findtext(\"key\")\n",
    "                v = entry.findtext(\"value\")\n",
    "                if k is not None:\n",
    "                    proc_info[\"properties\"][k] = v\n",
    "\n",
    "        processors.append(proc_info)\n",
    "\n",
    "    # Extract connections\n",
    "    for connection in root.findall(\".//connections\"):\n",
    "        source = _trim(connection.findtext(\".//source/id\") or \"Unknown\")\n",
    "        destination = _trim(connection.findtext(\".//destination/id\") or \"Unknown\")\n",
    "        rels = [\n",
    "            _trim(rel.text or \"\")\n",
    "            for rel in connection.findall(\".//selectedRelationships\")\n",
    "            if rel is not None and rel.text\n",
    "        ]\n",
    "        connections.append(\n",
    "            {\n",
    "                \"source\": source,\n",
    "                \"destination\": destination,\n",
    "                \"relationships\": rels,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return {\n",
    "        \"processors\": processors,\n",
    "        \"connections\": connections,\n",
    "        \"processor_count\": len(processors),\n",
    "        \"connection_count\": len(connections),\n",
    "        \"process_groups\": process_groups,\n",
    "    }\n",
    "\n",
    "\n",
    "def parse_nifi_template(xml_content: str) -> str:\n",
    "    \"\"\"\n",
    "    Parse a NiFi XML template and return JSON string.\n",
    "    Legacy interface for existing tool compatibility.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        result = parse_nifi_template_impl(xml_content)\n",
    "        result.update(\n",
    "            {\n",
    "                \"continue_required\": False,\n",
    "                \"tool_name\": \"parse_nifi_template\",\n",
    "            }\n",
    "        )\n",
    "        return json.dumps(result, indent=2)\n",
    "    except ET.ParseError as e:\n",
    "        return f\"Error parsing XML: {str(e)}\"\n",
    "    except Exception as e:\n",
    "        return f\"Unexpected error: {str(e)}\"\n",
    "    \n",
    "\n",
    "def list_xml_files(xml_volumes_path):\n",
    "    \"\"\"\n",
    "    Return a list of XML file paths from the \n",
    "    specified catalog, schema, and volume.\n",
    "    \"\"\"\n",
    "\n",
    "    xml_paths_df = (\n",
    "        spark.read.format(\"binaryFile\")\n",
    "            .option(\"recursiveFileLookup\", \"true\")\n",
    "            .option(\"pathGlobFilter\", \"*.xml\")       # only *.xml files\n",
    "            .load(xml_volumes_path)\n",
    "            .select(\"path\")                          # keep just the path\n",
    "            .distinct()\n",
    "    )\n",
    "\n",
    "    xml_paths_df.show(truncate=False)\n",
    "    # If you need them as a Python list:\n",
    "    xml_paths = [r.path for r in xml_paths_df.collect()]\n",
    "\n",
    "    return xml_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "875a6e9b-02db-4f13-8469-fa081ffd29ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, DateType, ArrayType\n",
    "\n",
    "def create_table(catalog, schema, table_name, table_schema):\n",
    "\n",
    "    df = spark.createDataFrame([], table_schema)\n",
    "\n",
    "    # Save the DataFrame as a managed Delta table\n",
    "    df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(f\"{catalog}.{schema}.{table_name}\")\n",
    "\n",
    "\n",
    "\n",
    "processor_schema = StructType([\n",
    "    StructField(\"id\", StringType(), False),\n",
    "    StructField(\"name\", StringType(), False),\n",
    "    StructField(\"type\", StringType(), False),\n",
    "    StructField(\"worflow_id\", StringType(), False),\n",
    "    StructField(\"workflow_name\", StringType(), False),\n",
    "    StructField(\"parent_group_id\", StringType(), True),\n",
    "    StructField(\"parent_group_name\", StringType(), True),\n",
    "    StructField(\"properties_id\", StringType(), True),\n",
    "    StructField(\"created_date\", DateType(), False),\n",
    "    StructField(\"last_updated_date\", DateType(), False)\n",
    "])\n",
    "\n",
    "processor_properties_schema = StructType([\n",
    "    StructField(\"processor_id\", StringType(), False),\n",
    "    StructField(\"property_name\", StringType(), False),\n",
    "    StructField(\"property_value\", StringType(), False),\n",
    "    StructField(\"property_rank\", StringType(), False),\n",
    "    StructField(\"created_date\", DateType(), False),\n",
    "    StructField(\"last_updated_date\", DateType(), False)\n",
    "])\n",
    "\n",
    "processor_connections_schema = StructType([\n",
    "    StructField(\"source_processor_id\", StringType(), False),\n",
    "    StructField(\"destination_processor_id\", StringType(), False),\n",
    "    StructField(\"relationships\", ArrayType(StringType()), True),\n",
    "    StructField(\"created_date\", DateType(), False),\n",
    "    StructField(\"last_updated_date\", DateType(), False)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d11b043-0647-419a-ae5e-f932f470f84f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Parse XML\n",
    "from pyspark.sql.functions import explode, current_date, lit\n",
    "from  pyspark.sql import functions as F\n",
    "\n",
    "xml_file_path = \"/Volumes/sdickey/nxp_nifi_flows_xml/raw_files/ICN8_NiFi_flows_2025-05-06.xml\" \n",
    "xml_volumes_path = \"/Volumes/sdickey/nxp_nifi_flows_xml/raw_files\"\n",
    "\n",
    "# Read the XML file contents\n",
    "with open(xml_file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "\n",
    "    xml_file_paths = list_xml_files(xml_volumes_path)\n",
    "    print(xml_file_paths)\n",
    "    \n",
    "    xml_content = f.read()\n",
    "\n",
    "    response = json.loads(parse_nifi_template(xml_content))\n",
    "\n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "\n",
    "    raw_processors_df = spark.createDataFrame(response[\"processors\"])\n",
    "    raw_connections_df = spark.createDataFrame(response[\"connections\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d0131c7c-abad-4860-bda8-16482b72d54a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# create_table(\"sdickey\", \"nifi_workflow_metadata\", \"nifi_processors\", processor_schema)\n",
    "\n",
    "# create_table(catalog, schema, \"nifi_processor_properties\", processor_properties_schema)\n",
    "\n",
    "# create_table(catalog, schema, \"nifi_connections\", processor_connections_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d8d1c6e-ab94-4bce-b135-614ba05cf88d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# 1. Base processors table\n",
    "processors_df = (\n",
    "    raw_processors_df\n",
    "    .selectExpr(\n",
    "        \"id\",\n",
    "        \"name\",\n",
    "        \"type\",\n",
    "        \"parentGroupId as parent_group_id\",\n",
    "        \"parentGroupName as parent_group_name\"\n",
    "    )\n",
    "    .withColumn(\"created_date\", current_date())\n",
    "    .withColumn(\"last_updated_date\", current_date())\n",
    ")\n",
    "    \n",
    "print(processors_df.count())\n",
    "processors_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1a9e06d-425e-4d67-a241-d753d5d05104",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 2. Properties table (explode the map into rows)\n",
    "processor_properties_df = (\n",
    "    raw_processors_df\n",
    "    .select(\n",
    "        F.col(\"id\").alias(\"processor_id\"),\n",
    "        F.posexplode_outer(\"properties\").alias(\"pos\", \"property_name\", \"property_value\")\n",
    "    )\n",
    "    .select(\n",
    "        \"processor_id\",\n",
    "        F.col(\"property_name\").alias(\"property_name\"),\n",
    "        F.col(\"property_value\").alias(\"property_value\"),\n",
    "        (F.col(\"pos\") + F.lit(1)).cast(\"string\").alias(\"property_rank\"),  # 1-based rank\n",
    "        F.current_date().alias(\"created_date\"),\n",
    "        F.current_date().alias(\"last_updated_date\"),\n",
    "    )\n",
    ")\n",
    "\n",
    "# print(processor_properties_df.count())\n",
    "processor_properties_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca1ef9ac-6ce2-4d87-bd7c-2f77858fbee3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "connections_df = (\n",
    "    raw_connections_df\n",
    "    .selectExpr(\n",
    "        \"source as source_processor_id\",\n",
    "        \"destination as destination_processor_id\",\n",
    "        \"relationships\"\n",
    "    )\n",
    "    .withColumn(\"created_date\", current_date())\n",
    "    .withColumn(\"last_updated_date\", current_date())\n",
    ")\n",
    "    \n",
    "print(connections_df.count())\n",
    "connections_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d18a6ba3-3bef-4611-a83a-d4ac26515634",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "raw_processors_df.createOrReplaceTempView(\"raw_processors\")\n",
    "processors_df.createOrReplaceTempView(\"processors\")\n",
    "processor_properties_df.createOrReplaceTempView(\"processor_properties\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "781fbc20-0974-4ae2-8f9a-a72192777ce5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "processor_query = \"select * from raw_processors where id = '0e44f1c7-17c4-3edf-0000-000000000000'\"\n",
    "tmp_df = spark.sql(processor_query)\n",
    "\n",
    "tmp_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6475767b-7a2d-4ff5-9c20-880e76c3b0c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "processor_query = \"select * from processors where id = '0e44f1c7-17c4-3edf-0000-000000000000'\"\n",
    "tmp_df = spark.sql(processor_query)\n",
    "\n",
    "tmp_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d36d97f1-ae0f-4860-b3cb-95ee3449d4b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "processor_properties = \"select * from processor_properties where processor_id = '0e44f1c7-17c4-3edf-0000-000000000000'\"\n",
    "tmp_df = spark.sql(processor_properties)\n",
    "\n",
    "tmp_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eda58877-b721-49af-80bf-4487c60c10cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# processors_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(f\"{catalog}.{schema}.nifi_processors\")\n",
    "# processor_properties_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(f\"{catalog}.{schema}.nifi_processor_properties\")\n",
    "# connections_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(f\"{catalog}.{schema}.nifi_connections\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "576504e1-3c95-4798-a669-cf22f6ec3f3b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "nifi_workflow_metadata_analyzer",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
