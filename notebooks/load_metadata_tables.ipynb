{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e489d6c-8154-44d2-99a2-f615c1aecbea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install databricks=0.2 databricks-langchain==0.5.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "53d64f12-b9c1-4c03-b69c-377c67076400",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import xml.etree.ElementTree as ET\n",
    "from typing import Any, Dict, List\n",
    "\n",
    "from schemas.nifi_metadata import processor_schema, processor_properties_schema, processor_connections_schema\n",
    "from tools.xml_tools import parse_nifi_template, list_xml_files\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import explode, current_date, lit\n",
    "from  pyspark.sql import functions as F\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "catalog = \"sdickey\"\n",
    "schema = \"nifi_workflow_metadata\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25ca02db-223d-41d8-bd07-44ece7052786",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "xml_file_path = \"/Volumes/sdickey/nxp_nifi_flows_xml/raw_files/ICN8_NiFi_flows_2025-05-06.xml\" \n",
    "xml_volumes_path = \"/Volumes/sdickey/nxp_nifi_flows_xml/raw_files\"\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "xml_file_paths = list_xml_files(xml_volumes_path)\n",
    "print(xml_file_paths)\n",
    "\n",
    "# Read the XML file contents\n",
    "with open(xml_file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    \n",
    "    xml_content = f.read()\n",
    "    response = json.loads(parse_nifi_template(xml_content))\n",
    "\n",
    "    print(response.keys())\n",
    "\n",
    "    raw_processors_df = spark.createDataFrame(response[\"processors\"])\n",
    "    raw_connections_df = spark.createDataFrame(response[\"connections\"])\n",
    "\n",
    "    # 1. Base processors table\n",
    "    processors_df = (\n",
    "        raw_processors_df\n",
    "        .selectExpr(\n",
    "            \"id\",\n",
    "            \"name\",\n",
    "            \"type\",\n",
    "            \"parentGroupId as parent_group_id\",\n",
    "            \"parentGroupName as parent_group_name\"\n",
    "        )\n",
    "        .withColumn(\"created_date\", current_date())\n",
    "        .withColumn(\"last_updated_date\", current_date())\n",
    "    )\n",
    "    \n",
    "    # 2. Properties table (explode the map into rows)\n",
    "    processor_properties_df = (\n",
    "        raw_processors_df\n",
    "        .select(\n",
    "            F.col(\"id\").alias(\"processor_id\"),\n",
    "            F.posexplode_outer(\"properties\").alias(\"pos\", \"property_name\", \"property_value\")\n",
    "        )\n",
    "        .select(\n",
    "            \"processor_id\",\n",
    "            F.col(\"property_name\").alias(\"property_name\"),\n",
    "            F.col(\"property_value\").alias(\"property_value\"),\n",
    "            (F.col(\"pos\") + F.lit(1)).cast(\"string\").alias(\"property_rank\"),\n",
    "            F.current_date().alias(\"created_date\"),\n",
    "            F.current_date().alias(\"last_updated_date\"),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # 2. connections table\n",
    "    connections_df = (\n",
    "        raw_connections_df\n",
    "        .selectExpr(\n",
    "            \"source as source_processor_id\",\n",
    "            \"destination as destination_processor_id\",\n",
    "            \"relationships\"\n",
    "        )\n",
    "        .withColumn(\"created_date\", current_date())\n",
    "        .withColumn(\"last_updated_date\", current_date())\n",
    "    )\n",
    "\n",
    "\n",
    "    processors_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(f\"{catalog}.{schema}.nifi_processors\")\n",
    "    processor_properties_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(f\"{catalog}.{schema}.nifi_processor_properties\")\n",
    "    connections_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(f\"{catalog}.{schema}.nifi_connections\")\n",
    "\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "57235a7b-5310-4f14-9da4-c07317ccd0d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "load_metadata_tables",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
