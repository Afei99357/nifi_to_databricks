{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Delta Tables from Hive DDL\n",
    "\n",
    "This notebook converts Hive/Impala DDL statements to Databricks Delta tables.\n",
    "\n",
    "**Features:**\n",
    "- Optional type optimization (STRING `_ts` columns → TIMESTAMP) - **disabled by default for safety**\n",
    "- Managed Delta tables with auto-optimization\n",
    "- Single file or batch processing\n",
    "- Dry-run mode to preview DDL\n",
    "\n",
    "**Prerequisites:**\n",
    "- Hive DDL files uploaded to Volumes\n",
    "- Appropriate permissions on target catalog/schema\n",
    "- Active cluster (serverless or all-purpose)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Import the table creation functions.\n",
    "\n",
    "**Note:** Update the path below to point to where you've cloned the repo in your workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Get the directory where this notebook is located\n",
    "notebook_path = dbutils.notebook.entry_point.getDbutils().notebook().getContext().notebookPath().get()\n",
    "notebook_dir = os.path.dirname(notebook_path)\n",
    "\n",
    "# Add the schema migration tool to Python path (assumes notebook is in the tool directory)\n",
    "sys.path.append(notebook_dir)\n",
    "\n",
    "from create_delta_tables import create_tables_from_hive_ddl\n",
    "\n",
    "print(\"✓ Imports successful\")\n",
    "print(f\"Working directory: {notebook_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Set your target catalog, schema, and file paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target Databricks catalog and schema\n",
    "TARGET_CATALOG = \"your_catalog\"  # UPDATE THIS\n",
    "TARGET_SCHEMA = \"your_schema\"    # UPDATE THIS\n",
    "\n",
    "print(f\"Target location: {TARGET_CATALOG}.{TARGET_SCHEMA}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 1: Single File Processing\n",
    "\n",
    "Convert a single Hive DDL file to Delta table.\n",
    "\n",
    "**Note:** By default, column types are kept as-is (no automatic conversion)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single file - UPDATE the path to your DDL file\n",
    "SINGLE_FILE_PATH = \"/Volumes/your_catalog/your_schema/your_path/table.sql\"  # UPDATE THIS\n",
    "\n",
    "result = create_tables_from_hive_ddl(\n",
    "    input_file=SINGLE_FILE_PATH,\n",
    "    catalog=TARGET_CATALOG,\n",
    "    schema=TARGET_SCHEMA\n",
    "    # optimize_types=False is the default (keeps original types, _ts as STRING)\n",
    "    # If you want to convert _ts STRING columns to TIMESTAMP, set optimize_types=True\n",
    ")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"RESULT\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"✓ Successfully created: {result['success_count']} table(s)\")\n",
    "print(f\"✗ Failed: {result['fail_count']} table(s)\")\n",
    "print(f\"Total processed: {result['total']} file(s)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single File - Dry Run\n",
    "\n",
    "Preview the DDL without creating tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dry run - just show what would be created\n",
    "result = create_tables_from_hive_ddl(\n",
    "    input_file=SINGLE_FILE_PATH,\n",
    "    catalog=TARGET_CATALOG,\n",
    "    schema=TARGET_SCHEMA,\n",
    "    dry_run=True  # Only preview, don't create\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 2: Batch Processing\n",
    "\n",
    "Process multiple DDL files from a directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch processing - UPDATE the path to your directory containing .sql files\n",
    "BATCH_DIRECTORY = \"/Volumes/your_catalog/your_schema/your_path/hive_ddls/\"  # UPDATE THIS\n",
    "\n",
    "result = create_tables_from_hive_ddl(\n",
    "    input_dir=BATCH_DIRECTORY,\n",
    "    catalog=TARGET_CATALOG,\n",
    "    schema=TARGET_SCHEMA\n",
    ")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"BATCH PROCESSING RESULT\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"✓ Successfully created: {result['success_count']} table(s)\")\n",
    "print(f\"✗ Failed: {result['fail_count']} table(s)\")\n",
    "print(f\"Total processed: {result['total']} file(s)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Processing - Dry Run\n",
    "\n",
    "Preview all tables without creating them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dry run for batch - see DDL for all files\n",
    "result = create_tables_from_hive_ddl(\n",
    "    input_dir=BATCH_DIRECTORY,\n",
    "    catalog=TARGET_CATALOG,\n",
    "    schema=TARGET_SCHEMA,\n",
    "    dry_run=True  # Only preview, don't create\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify Created Tables\n",
    "\n",
    "Check that tables were created successfully.\n",
    "\n",
    "**Note:** Update the catalog and schema names in the SQL queries below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Show all tables in the target schema (UPDATE catalog and schema names)\n",
    "SHOW TABLES IN your_catalog.your_schema;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Describe a specific table (UPDATE catalog, schema, and table names)\n",
    "DESCRIBE EXTENDED your_catalog.your_schema.your_table;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WITH Type Optimization (Use with Caution)\n",
    "\n",
    "⚠️ **WARNING:** Only enable if you're 100% certain your naming convention uses `_ts` suffix exclusively for timestamps.\n",
    "\n",
    "This will convert STRING columns ending with `_ts` to TIMESTAMP.\n",
    "\n",
    "**Risk:** Column names like `counts`, `status_ts`, `bytes` would be incorrectly converted!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process WITH type optimization (risky!)\n",
    "result = create_tables_from_hive_ddl(\n",
    "    input_file=SINGLE_FILE_PATH,\n",
    "    catalog=TARGET_CATALOG,\n",
    "    schema=TARGET_SCHEMA,\n",
    "    optimize_types=True  # ⚠️ Converts _ts STRING columns to TIMESTAMP\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Catalog and Schema\n",
    "\n",
    "Create tables in different catalog/schema combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create in dev environment\n",
    "result_dev = create_tables_from_hive_ddl(\n",
    "    input_file=SINGLE_FILE_PATH,\n",
    "    catalog=\"dev\",\n",
    "    schema=\"bronze\"\n",
    ")\n",
    "\n",
    "# Create in prod environment\n",
    "result_prod = create_tables_from_hive_ddl(\n",
    "    input_file=SINGLE_FILE_PATH,\n",
    "    catalog=\"prod\",\n",
    "    schema=\"bronze\"\n",
    ")\n",
    "\n",
    "print(f\"Dev: {result_dev['success_count']} tables created\")\n",
    "print(f\"Prod: {result_prod['success_count']} tables created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Troubleshooting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check if schema exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql\n",
    "-- UPDATE catalog name\n",
    "SHOW SCHEMAS IN your_catalog;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check catalog permissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql\n",
    "-- UPDATE catalog name\n",
    "SHOW GRANTS ON CATALOG your_catalog;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View table properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql\n",
    "-- View Delta table properties (UPDATE catalog, schema, and table names)\n",
    "SHOW TBLPROPERTIES your_catalog.your_schema.your_table;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "\n",
    "- Tables are created as **managed Delta tables** (no LOCATION clause)\n",
    "- Partitioning is preserved from the Hive table\n",
    "- Auto-optimization is enabled by default\n",
    "- Tables are created **empty** (structure only, no data)\n",
    "- **Column types are preserved by default** - no automatic conversion\n",
    "- `_ts` columns remain as STRING unless you explicitly set `optimize_types=True`\n",
    "- To load data, use separate `INSERT INTO` or data migration tools"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
