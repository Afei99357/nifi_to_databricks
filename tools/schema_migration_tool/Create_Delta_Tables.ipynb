{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Delta Tables from Hive DDL\n",
    "\n",
    "This notebook converts Hive/Impala DDL statements to Databricks Delta tables.\n",
    "\n",
    "**What it does:**\n",
    "- Reads Hive DDL files (.sql)\n",
    "- Creates managed Delta tables in Databricks\n",
    "- Preserves table structure, partitioning, and column types\n",
    "- Creates empty tables (structure only, no data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Get notebook directory and add to path\n",
    "notebook_path = dbutils.notebook.entry_point.getDbutils().notebook().getContext().notebookPath().get()\n",
    "sys.path.append(os.path.dirname(notebook_path))\n",
    "\n",
    "from create_delta_tables import create_tables_from_hive_ddl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "**Update these values:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target catalog and schema\n",
    "CATALOG = \"your_catalog\"  # UPDATE\n",
    "SCHEMA = \"your_schema\"    # UPDATE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single File\n",
    "\n",
    "Create one table from a single DDL file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to your Hive DDL file\n",
    "FILE_PATH = \"/Volumes/catalog/schema/path/table.sql\"  # UPDATE\n",
    "\n",
    "result = create_tables_from_hive_ddl(\n",
    "    input_file=FILE_PATH,\n",
    "    catalog=CATALOG,\n",
    "    schema=SCHEMA\n",
    ")\n",
    "\n",
    "print(f\"✓ Created {result['success_count']} table(s)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Process\n",
    "\n",
    "Create multiple tables from a directory of DDL files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to directory containing .sql files\n",
    "DIRECTORY = \"/Volumes/catalog/schema/path/hive_ddls/\"  # UPDATE\n",
    "\n",
    "result = create_tables_from_hive_ddl(\n",
    "    input_dir=DIRECTORY,\n",
    "    catalog=CATALOG,\n",
    "    schema=SCHEMA\n",
    ")\n",
    "\n",
    "print(f\"✓ Created {result['success_count']} table(s)\")\n",
    "print(f\"✗ Failed {result['fail_count']} table(s)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify\n",
    "\n",
    "Check created tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql\n",
    "SHOW TABLES IN your_catalog.your_schema;  -- UPDATE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "\n",
    "- Tables are created **empty** (structure only)\n",
    "- Column types are preserved as-is from Hive\n",
    "- Tables are managed Delta tables with auto-optimization\n",
    "- To preview DDL before creating, add `dry_run=True` parameter"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
