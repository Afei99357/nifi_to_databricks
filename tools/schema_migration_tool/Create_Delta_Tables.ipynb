{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Delta Tables from Hive DDL\n",
    "\n",
    "This notebook converts Hive/Impala DDL statements to Databricks Delta tables.\n",
    "\n",
    "**Features:**\n",
    "- Optional type optimization (STRING `_ts` columns → TIMESTAMP) - **disabled by default for safety**\n",
    "- Managed Delta tables with auto-optimization\n",
    "- Single file or batch processing\n",
    "- Dry-run mode to preview DDL\n",
    "\n",
    "**Prerequisites:**\n",
    "- Hive DDL files uploaded to Volumes\n",
    "- Appropriate permissions on target catalog/schema\n",
    "- Active cluster (serverless or all-purpose)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Import the table creation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# Add the schema migration tool to Python path\n",
    "sys.path.append(\"/Workspace/Users/eliao@bpcs.com/nifi_to_databricks_test/tools/schema_migration_tool\")\n",
    "\n",
    "from create_delta_tables import create_tables_from_hive_ddl\n",
    "\n",
    "print(\"✓ Imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Set your target catalog and schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target Databricks catalog and schema\n",
    "TARGET_CATALOG = \"eliao\"\n",
    "TARGET_SCHEMA = \"nifi_to_databricks\"\n",
    "\n",
    "print(f\"Target location: {TARGET_CATALOG}.{TARGET_SCHEMA}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 1: Single File Processing\n",
    "\n",
    "Convert a single Hive DDL file to Delta table.\n",
    "\n",
    "**Note:** By default, column types are kept as-is (no automatic conversion)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single file - update the path to your DDL file\n",
    "SINGLE_FILE_PATH = \"/Volumes/eliao/nifi_to_databricks/test_data_files/test.sql\"\n",
    "\n",
    "result = create_tables_from_hive_ddl(\n",
    "    input_file=SINGLE_FILE_PATH,\n",
    "    catalog=TARGET_CATALOG,\n",
    "    schema=TARGET_SCHEMA\n",
    "    # optimize_types=False is the default (keeps original types)\n",
    ")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"RESULT\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"✓ Successfully created: {result['success_count']} table(s)\")\n",
    "print(f\"✗ Failed: {result['fail_count']} table(s)\")\n",
    "print(f\"Total processed: {result['total']} file(s)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single File - Dry Run\n",
    "\n",
    "Preview the DDL without creating tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dry run - just show what would be created\n",
    "result = create_tables_from_hive_ddl(\n",
    "    input_file=SINGLE_FILE_PATH,\n",
    "    catalog=TARGET_CATALOG,\n",
    "    schema=TARGET_SCHEMA,\n",
    "    dry_run=True  # Only preview, don't create\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 2: Batch Processing\n",
    "\n",
    "Process multiple DDL files from a directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch processing - update the path to your directory containing .sql files\n",
    "BATCH_DIRECTORY = \"/Volumes/eliao/nifi_to_databricks/hive_ddls/\"\n",
    "\n",
    "result = create_tables_from_hive_ddl(\n",
    "    input_dir=BATCH_DIRECTORY,\n",
    "    catalog=TARGET_CATALOG,\n",
    "    schema=TARGET_SCHEMA\n",
    ")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"BATCH PROCESSING RESULT\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"✓ Successfully created: {result['success_count']} table(s)\")\n",
    "print(f\"✗ Failed: {result['fail_count']} table(s)\")\n",
    "print(f\"Total processed: {result['total']} file(s)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Processing - Dry Run\n",
    "\n",
    "Preview all tables without creating them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dry run for batch - see DDL for all files\n",
    "result = create_tables_from_hive_ddl(\n",
    "    input_dir=BATCH_DIRECTORY,\n",
    "    catalog=TARGET_CATALOG,\n",
    "    schema=TARGET_SCHEMA,\n",
    "    dry_run=True  # Only preview, don't create\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify Created Tables\n",
    "\n",
    "Check that tables were created successfully"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Show all tables in the target schema\n",
    "SHOW TABLES IN eliao.nifi_to_databricks;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Describe a specific table (update table name)\n",
    "DESCRIBE EXTENDED eliao.nifi_to_databricks.obf_table_raw;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WITH Type Optimization (Use with Caution)\n",
    "\n",
    "⚠️ **WARNING:** Only enable if you're 100% certain your naming convention uses `_ts` suffix exclusively for timestamps.\n",
    "\n",
    "This will convert STRING columns ending with `_ts` to TIMESTAMP.\n",
    "\n",
    "**Risk:** Column names like `counts`, `status_ts`, `bytes` would be incorrectly converted!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process WITH type optimization (risky!)\n",
    "result = create_tables_from_hive_ddl(\n",
    "    input_file=SINGLE_FILE_PATH,\n",
    "    catalog=TARGET_CATALOG,\n",
    "    schema=TARGET_SCHEMA,\n",
    "    optimize_types=True  # ⚠️ Converts _ts STRING columns to TIMESTAMP\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Catalog and Schema\n",
    "\n",
    "Create tables in different catalog/schema combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create in dev environment\n",
    "result_dev = create_tables_from_hive_ddl(\n",
    "    input_file=SINGLE_FILE_PATH,\n",
    "    catalog=\"dev\",\n",
    "    schema=\"bronze\"\n",
    ")\n",
    "\n",
    "# Create in prod environment\n",
    "result_prod = create_tables_from_hive_ddl(\n",
    "    input_file=SINGLE_FILE_PATH,\n",
    "    catalog=\"prod\",\n",
    "    schema=\"bronze\"\n",
    ")\n",
    "\n",
    "print(f\"Dev: {result_dev['success_count']} tables created\")\n",
    "print(f\"Prod: {result_prod['success_count']} tables created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Troubleshooting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check if schema exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql\n",
    "SHOW SCHEMAS IN eliao LIKE 'nifi*';"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check catalog permissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql\n",
    "SHOW GRANTS ON CATALOG eliao;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View table properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql\n",
    "-- View Delta table properties\n",
    "SHOW TBLPROPERTIES eliao.nifi_to_databricks.obf_table_raw;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "\n",
    "- Tables are created as **managed Delta tables** (no LOCATION clause)\n",
    "- Partitioning is preserved from the Hive table\n",
    "- Auto-optimization is enabled by default\n",
    "- Tables are created **empty** (structure only, no data)\n",
    "- **Column types are preserved by default** - no automatic conversion\n",
    "- To load data, use separate `INSERT INTO` or data migration tools"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
