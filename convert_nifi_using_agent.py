# Databricks notebook source
# MAGIC %pip install -U -qqqq backoff uv json-repair

# COMMAND ----------

dbutils.library.restartPython()

# COMMAND ----------

from datetime import datetime

from tools.simplified_migration import (
    analyze_nifi_workflow_only,
    migrate_nifi_to_databricks_simplified,
)

# COMMAND ----------

# MAGIC %md
# MAGIC ## ğŸš€ Simplified NiFi to Databricks Migration
# MAGIC
# MAGIC This notebook demonstrates the simplified direct function approach:
# MAGIC - **Direct Function Calls**: No agent complexity
# MAGIC - **Linear Pipeline**: analyze â†’ prune â†’ chain â†’ migrate
# MAGIC - **Complete Migration**: Generates production-ready Databricks assets

# COMMAND ----------

# MAGIC %md
# MAGIC ### Complete NiFi to Databricks Migration Example
# MAGIC
# MAGIC This example shows the complete migration process from a NiFi XML file to production-ready Databricks assets.

# COMMAND ----------

# Configuration
current = datetime.now().strftime("%Y%m%d%H%M%S")
xml_path = "/Volumes/eliao/nifi_to_databricks/nifi_files/ICN8_BRS_Feedback.xml"
output_dir = (
    "/Workspace/Users/eliao@bpcs.com/nifi_to_databricks_large_xml/output_results"
)
project_name = f"nifi_migration_{current}"

print("ğŸš€ COMPLETE NIFI TO DATABRICKS MIGRATION")
print("=" * 60)
print(f"ğŸ“ Input: {xml_path}")
print(f"ğŸ“‚ Output: {output_dir}/{project_name}")
print("=" * 60)

# COMMAND ----------

# MAGIC %md
# MAGIC #### Step 1: Quick Analysis (Optional)
# MAGIC
# MAGIC First, let's understand what's in the NiFi workflow before migration.

# COMMAND ----------

print("ğŸ” STEP 1: ANALYZING NIFI WORKFLOW")
print("-" * 40)

# Quick analysis to understand the workflow
analysis_result = analyze_nifi_workflow_only(xml_path)

print("âœ… Analysis complete!")
print(f"ğŸ“Š Workflow contains semantic data flows and processor classifications")
print(f"ğŸ¯ Ready for intelligent migration")

# COMMAND ----------

# MAGIC %md
# MAGIC #### Step 2: Complete Migration
# MAGIC
# MAGIC Execute the full migration pipeline: analyze â†’ prune â†’ chain â†’ migrate

# COMMAND ----------

print("ğŸš€ STEP 2: COMPLETE MIGRATION PIPELINE")
print("-" * 40)

# Execute complete migration with all intelligence features
migration_result = migrate_nifi_to_databricks_simplified(
    xml_path=xml_path,
    out_dir=output_dir,
    project=project_name,
    notebook_path=f"{output_dir}/{project_name}/main",
    deploy=False,  # Set to True to automatically deploy the job
)

print("âœ… MIGRATION COMPLETED SUCCESSFULLY!")
print("=" * 60)

# COMMAND ----------

# MAGIC %md
# MAGIC #### Step 3: Review Results

# COMMAND ----------

print("ğŸ“‹ MIGRATION RESULTS SUMMARY")
print("=" * 60)

# Display migration results
print(f"âœ… Status: Success")
print(f"ğŸ“ Output Directory: {migration_result['configuration']['out_dir']}")
print(f"ğŸ·ï¸  Project: {migration_result['configuration']['project']}")
print(f"ğŸ““ Notebook Path: {migration_result['configuration']['notebook_path']}")

print("\nğŸ” ANALYSIS BREAKDOWN:")
analysis = migration_result["analysis"]
print(f"ğŸ“Š Workflow Analysis: {analysis.get('workflow_analysis', 'Completed')}")
print(f"ğŸ·ï¸  Processor Classifications: Available")
print(f"âœ‚ï¸  Pruning Results: Infrastructure processors removed")
print(f"ğŸ”— Data Flow Chains: Semantic chains detected")
print(f"ğŸŒŠ Semantic Flows: Business flows created")

print(f"\nğŸ“ Generated Assets:")
print(f"   â€¢ src/steps/ - Individual processor Python files")
print(f"   â€¢ notebooks/ - Orchestrator notebook")
print(f"   â€¢ jobs/ - Databricks job configurations")
print(f"   â€¢ conf/ - Migration plans and configurations")
print(f"   â€¢ databricks.yml - Asset bundle for deployment")
print(f"   â€¢ README.md - Documentation and next steps")

print("=" * 60)
print("ğŸ‰ Your NiFi workflow is now ready to run on Databricks!")
print("ğŸ“– Check the generated README.md for deployment instructions")

# COMMAND ----------

# MAGIC %md
# MAGIC ## ğŸ“ Next Steps
# MAGIC
# MAGIC 1. **Review Generated Assets**: Check the output directory for all created files
# MAGIC 2. **Test the Migration**: Run the generated notebook to verify functionality
# MAGIC 3. **Deploy to Production**: Use `deploy=True` or follow README.md instructions
# MAGIC 4. **Customize as Needed**: Modify generated PySpark code for your specific requirements
# MAGIC
# MAGIC ### ğŸ”§ To Deploy Automatically:
# MAGIC ```python
# MAGIC # Re-run with deployment enabled
# MAGIC migrate_nifi_to_databricks_simplified(
# MAGIC     xml_path=xml_path,
# MAGIC     out_dir=output_dir,
# MAGIC     project=project_name,
# MAGIC     deploy=True  # This will create and run the Databricks job
# MAGIC )
# MAGIC ```
# MAGIC
# MAGIC **That's it! Your NiFi workflow is now a production-ready Databricks pipeline! ğŸš€**
